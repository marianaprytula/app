import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


HUGGINGFACE_TOKEN = st.secrets["HUGGINGFACE_TOKEN"]


BASE_MODEL = "meta-llama/Llama-3.1-8B-Instruct"
ADAPTER_PATH = "Marivanna27/fine-tuned-model_llama3_1_binary"


@st.cache_resource
def load_model():
    try:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,  # Enable 4-bit quantization
            bnb_4bit_compute_dtype=torch.float16,  # Compute in float16
            bnb_4bit_use_double_quant=True,  # Further reduce memory usage
            bnb_4bit_quant_type="nf4"  # Use Normal Float 4 (NF4) quantization
        )

        st.write("Loading tokenizer")
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HUGGINGFACE_TOKEN)

        st.write("‚è≥ Loading base model with 4-bit quantization (this may take time)...")
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            token=HUGGINGFACE_TOKEN,
            quantization_config=quantization_config,
            device_map="auto"  # Auto-detect GPU if available
        )

        st.write("Loading LoRA adapter")
        model = PeftModel.from_pretrained(model, ADAPTER_PATH)

        return model, tokenizer
    except Exception as e:
        st.error(f"üö® Error loading model: {str(e)}")
        return None, None

# Load model & tokenizer
model, tokenizer = load_model()



st.title("LLaMA3 –±—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç–µ–∫—Å—Ç—É")
st.write("–í–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç —Ç–∞ —Ç–µ–∫—Å—Ç, —è–∫–∏–π –ø–æ—Ç—Ä—ñ–±–Ω–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏. –í—ñ–¥–ø–æ–≤—ñ–¥—å 0 –≤–∫–∞–∑—É—î, —â–æ —Ç–µ–∫—Å—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π, –∞ 1 - —è–∫—â–æ —Ç–µ–∫—Å—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π.")
st.write("–í–∞—Ä—ñ–∞–Ω—Ç–∏ –ø—Ä–æ–ø–º—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π:")
st.write("""–í–∏–∑–Ω–∞—á –∑–∞–≥–∞–ª—å–Ω—É —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –ª–∏—à–µ –æ–¥–Ω—É –∑ –º–æ–∂–ª–∏–≤–∏—Ö —Ü–∏—Ñ—Ä: 0, –∞–±–æ 1. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π 0, —è–∫—â–æ –∫–æ–º–µ–Ω—Ç–∞—Ä —î –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–º, –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–π 1, —è–∫—â–æ –∫–æ–º–µ–Ω—Ç–∞—Ä —î –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º. 
            –ö–æ–º–µ–Ω—Ç–∞—Ä:""")
st.write("""–¢–∏ —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç–µ–∫—Å—Ç—ñ–≤. 
        –Ø–∫—â–æ –≤ —Ç–µ–∫—Å—Ç—ñ –≤–∏—è–≤–ª–µ–Ω–æ –æ–¥–Ω–æ—á–∞—Å–Ω–æ —ñ –ø–æ–∑–∏—Ç–∏–≤–Ω—É, —ñ –Ω–µ–≥–∞—Ç–∏–≤–Ω—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ, —Ç–æ–¥—ñ –≤–∏–∑–Ω–∞—á, —è–∫–∞ –∑ –Ω–∏—Ö —î –¥–æ–º—ñ–Ω—É—é—á–æ—é —ñ –ø–æ–¥–∞–π —ó—ó —è–∫ –æ—Å–Ω–æ–≤–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π 0, —è–∫—â–æ –∫–æ–º–µ–Ω—Ç–∞—Ä —î –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–º, –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–π 1, —è–∫—â–æ –∫–æ–º–µ–Ω—Ç–∞—Ä —î –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º. 
        –§–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω–µ —á–∏—Å–ª–æ (0 –∞–±–æ 1).
        –ü–∏—Ç–∞–Ω–Ω—è - –í–∏–∑–Ω–∞—á –∑–∞–≥–∞–ª—å–Ω—É —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—è: """)




if st.button("–ü—Ä–æ–≤–µ—Å—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—é"):
    if user_input:
        # Tokenize input
        input_ids = tokenizer(user_input, return_tensors="pt")


        outputs = model.generate(**inputs, max_length=inputs["input_ids"].shape[1] + 2)

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        
        # Display the result
        st.write("### –¢–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å:")
        st.write(generated_text)
    else:
        st.warning("–í–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç!")
