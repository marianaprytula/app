import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


HUGGINGFACE_TOKEN = st.secrets["HUGGINGFACE_TOKEN"]




@st.cache_resource
def load_model():
    try:
        st.write("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

        st.write("Loading base model (this can take several minutes)...")
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            device_map="auto",
            torch_dtype=torch.float16  # Speeds up loading
        )

        st.write("‚è≥ Loading LoRA adapter...")
        model = PeftModel.from_pretrained(model, ADAPTER_PATH)

        st.success("‚úÖ Model and adapter loaded successfully!")
        return model, tokenizer
    except Exception as e:
        st.error(f"üö® Error loading model: {str(e)}")
        return None, None

model, tokenizer = load_model()


st.title("LLaMA3 –±—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç–µ–∫—Å—Ç—É")
st.write("–í–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç —Ç–∞ —Ç–µ–∫—Å—Ç, —è–∫–∏–π –ø–æ—Ç—Ä—ñ–±–Ω–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏. –í—ñ–¥–ø–æ–≤—ñ–¥—å 0 –≤–∫–∞–∑—É—î, —â–æ —Ç–µ–∫—Å—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π, –∞ 1 - —è–∫—â–æ —Ç–µ–∫—Å—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π.")
st.write()
st.write("üü° –í–∞—Ä—ñ–∞–Ω—Ç–∏ –ø—ñ–¥–∫–∞–∑–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ:")
st.write("""1Ô∏è‚É£ –í–∏–∑–Ω–∞—á –∑–∞–≥–∞–ª—å–Ω—É —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –ª–∏—à–µ –æ–¥–Ω—É –∑ –º–æ–∂–ª–∏–≤–∏—Ö —Ü–∏—Ñ—Ä: 0, –∞–±–æ 1. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π 0, —è–∫—â–æ –∫–æ–º–µ–Ω—Ç–∞—Ä —î –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–º, –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–π 1, —è–∫—â–æ –∫–æ–º–µ–Ω—Ç–∞—Ä —î –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º. 
            –ö–æ–º–µ–Ω—Ç–∞—Ä:""")
st.write("""2Ô∏è‚É£ –¢–∏ —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç–µ–∫—Å—Ç—ñ–≤. 
        –Ø–∫—â–æ –≤ —Ç–µ–∫—Å—Ç—ñ –≤–∏—è–≤–ª–µ–Ω–æ –æ–¥–Ω–æ—á–∞—Å–Ω–æ —ñ –ø–æ–∑–∏—Ç–∏–≤–Ω—É, —ñ –Ω–µ–≥–∞—Ç–∏–≤–Ω—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ, —Ç–æ–¥—ñ –≤–∏–∑–Ω–∞—á, —è–∫–∞ –∑ –Ω–∏—Ö —î –¥–æ–º—ñ–Ω—É—é—á–æ—é —ñ –ø–æ–¥–∞–π —ó—ó —è–∫ –æ—Å–Ω–æ–≤–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π 0, —è–∫—â–æ –∫–æ–º–µ–Ω—Ç–∞—Ä —î –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–º, –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–π 1, —è–∫—â–æ –∫–æ–º–µ–Ω—Ç–∞—Ä —î –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º. 
        –§–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω–µ —á–∏—Å–ª–æ (0 –∞–±–æ 1).
        –ü–∏—Ç–∞–Ω–Ω—è - –í–∏–∑–Ω–∞—á –∑–∞–≥–∞–ª—å–Ω—É —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—è: """)

user_input = st.text_area("‚úèÔ∏è –í–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç:", "")



if st.button("‚è≥–ü—Ä–æ–≤–µ—Å—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—é"):
    if user_input:
        # Tokenize input
        input_ids = tokenizer(user_input, return_tensors="pt")


        outputs = model.generate(**inputs, max_length=inputs["input_ids"].shape[1] + 2)

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        
        # Display the result
        st.write("### ‚úÖ –¢–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å:")
        st.write(generated_text)
    else:
        st.warning("üö®–í–≤–µ–¥—ñ—Ç—å –ø—Ä–æ–º–ø—Ç!")
